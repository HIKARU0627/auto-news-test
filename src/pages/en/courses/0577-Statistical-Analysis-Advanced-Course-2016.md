---
# テンプレート指定
templateKey: "courses-en"

# コースID
course_id: "0577"

# タイトル
title: "Statistical Analysis Advanced Course"

# 簡単な説明
description: >-
  Course Contents1. Review of probability theory: probability, conditional probability, expectation value, dispersion and other basics.2. Linear regression: least-square method, regularization, and c ....
# 講師名
lecturer: "Takafumi KANAMORI, Associate Professor"

# 部局名
department: "Graduate School of Information Science"

# 開講時限
term: "2016	Spring"

# 対象者、単位数、授業回数（修正用の元データ）
class_is_for: "計算機数理科学専攻、2単位、週1回全15回"

# 対象者
target: "Department of Computer Science and Mathematical Informatics"

# 授業回数
classes: "週1回全15回"

# 単位数
credit: "2"

# pdfなどの追加資料
attachments:
  - name: "情報科学　金森先生　講義資料"
    path: https://ocw.nagoya-u.jp/files/577/01_prob(1).pdf
  - name: "情報科学　金森先生　講義資料"
    path: https://ocw.nagoya-u.jp/files/577/02_regression(2).pdf
  - name: "情報科学　金森先生　講義資料"
    path: https://ocw.nagoya-u.jp/files/577/03_RKHS(1).pdf
  - name: "情報科学　金森先生　講義資料"
    path: https://ocw.nagoya-u.jp/files/577/04_classificatiion(1).pdf
  - name: "情報科学　金森先生　講義資料"
    path: https://ocw.nagoya-u.jp/files/577/05_LearnAlg(1).pdf
  - name: "情報科学　金森先生　講義資料"
    path: https://ocw.nagoya-u.jp/files/577/exam.pdf
  - name: "情報科学　金森先生　講義資料"
    path: https://ocw.nagoya-u.jp/files/577/report1.pdf
  - name: "情報科学　金森先生　講義資料"
    path: https://ocw.nagoya-u.jp/files/577/report2.pdf
  - name: "情報科学　金森先生　講義資料"
    path: https://ocw.nagoya-u.jp/files/577/report3.pdf

# 関連するタグ
# （頻度の高い単語を出力）
tags:
  - kernel
  - analysis
  - machine
  - information
# カテゴリ
category:
  - "Information science and culture"

# 色付けのロールにするか
featuredpost: true

# 画像
## rootフォルダはstaticになっている
## なにも指定がない場合はデフォルトの画像が表示される
## 映像がある場合は映像優先で表示する
featuredimage:

# 映像のURL
## なにも指定がない場合は画像が表示される
movie:

# 記事投稿日
date: 2018-05-23 11:50:47
---

### Course Contents

1. Review of probability theory: probability, conditional probability, expectation value, dispersion and other basics.
2. Linear regression: least-square method, regularization, and cross-validation method.
3. Kernel method: kernel regression analysis, and reproducing kernel Hilbert space.
4. Discriminant analysis: Bayes' rule and bias-variance tradeoff.
5. Support vector machine: algorithm, kernel SVM, and multi-classification.

### Course Features

In order for students to learn smoothly, all materials given in class will be available online. Also, submitted reports will be marked and returned in a week or so to promote self-study. There will be a specific emphasis on mathematical aspects in this class.
The aim of this class is to be able to clearly express the train of thought used in information science, which is different from the conventional scientific paradigm.

### Course Aims

To learn theoretical basics of machine learning and mathematical statistics.

### Textbooks

None. Handouts will be provided.

### References

Mohri, et al., Foundations of Machine Learning, The MIT Press, 2012.

<h3>Schedule</h3>
<table class="basic" width="455">
<tr>
<th width="20" class="center">Session</th>
<th width="435">Contents</th>
</tr>
<tr>
<td width="20" class="center">1</td>
<td width="435">Introduction, review of probability and statistics</td>
</tr>
<tr>
<td width="20" class="center"bgcolor="#EAE2E2">2</td>
<td width="435"bgcolor="#EAE2E2">Linear regression: least-square method</td>
</tr>
<tr>
<td width="20" class="center">3</td>
<td width="435">Properties of least-square method and cross-validation method</td>
</tr>
<tr>
<td width="20" class="center"bgcolor="#EAE2E2">4</td>
<td width="435"bgcolor="#EAE2E2">High dimensional model and regularization</td>
</tr>
<tr>
<td width="20" class="center">5</td>
<td width="435">Kernel regression analysis</td>
</tr>
<tr>
<td width="20" class="center"bgcolor="#EAE2E2">6</td>
<td width="435"bgcolor="#EAE2E2">Positive-definite kernel</td>
</tr>
<tr>
<td width="20" class="center">7</td>
<td width="435">Reproducing kernel Hilbert space</td>
</tr>
<tr>
<td width="20" class="center"bgcolor="#EAE2E2">8</td>
<td width="435"bgcolor="#EAE2E2">Discriminant analysis</td>
</tr>
<tr>
<td width="20" class="center">9</td>
<td width="435">Bayes' rule</td>
</tr>
<tr>
<td width="20" class="center"bgcolor="#EAE2E2">10</td>
<td width="435"bgcolor="#EAE2E2">Evaluation of prediction error and bias-variance tradeoff</td>
</tr>
<tr>
<td width="20" class="center">11</td>
<td width="435">Surrogate loss</td>
</tr>
<tr>
<td width="20" class="center"bgcolor="#EAE2E2">12</td>
<td width="435"bgcolor="#EAE2E2">Support vector machine (SVM): Algorithm</td>
</tr>
<tr>
<td width="20" class="center">13</td>
<td width="435">Support vector machine (SVM): Statistical property</td>
</tr>
<tr>
<td width="20" class="center"bgcolor="#EAE2E2">14</td>
<td width="435"bgcolor="#EAE2E2">Kernel SVM, multiple variable SVM</td>
</tr>
<tr>
<td width="20" class="center">15</td>
<td width="435"> Multi-class classification based on error correcting output coding (ECOC) method </td>
</tr>
</table>

### Grade evaluation

Grades are based on the final exam and reports.

---
